\documentclass[twoside,final]{hcmut-report}

\usepackage[utf8]{inputenc}
\usepackage[T5]{fontenc}
\usepackage[protrusion=false]{microtype}

\usepackage{graphicx, caption}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{multirow,multicol}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tcolorbox}
\usepackage{lastpage}

\definecolor{keywordcolor}{rgb}{0.13,0.13,1} % Blue keywords
\definecolor{stringcolor}{rgb}{0.7,0.13,0.13} % Dark red strings
\definecolor{commentcolor}{rgb}{0.25,0.5,0.35} % Green comments
\definecolor{backgroundcolor}{rgb}{0.97,0.97,0.97} % Light background

\lstdefinelanguage{Python}{
  morekeywords={
    and, as, assert, async, await, break, class, continue, def, del, elif, else, except,
    False, finally, for, from, global, if, import, in, is, lambda, None, nonlocal, not, 
    or, pass, raise, return, True, try, while, with, yield
  },
  keywordstyle=\color{keywordcolor}\bfseries,
  morekeywords=[2]{bool, bytes, bytearray, complex, dict, float, frozenset, int, list, object, set, str, tuple},
  keywordstyle=[2]\color{blue}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{\#},
  morecomment=[s]{'''}{'''}, 
  morecomment=[s]{"""}{"""},
  commentstyle=\color{commentcolor}\ttfamily,
  stringstyle=\color{stringcolor}\ttfamily,
  morestring=[b]',
  morestring=[b]",
}
\lstset{
  language=Python,
  backgroundcolor=\color{backgroundcolor},
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  frame=single,
}

\lstdefinelanguage{TypeScript}{
  keywords={abstract, as, boolean, break, case, catch, class, continue, const, constructor, debugger, declare, default, delete, do, else, enum, export, extends, false, finally, for, from, function, get, if, implements, import, in, infer, instanceof, interface, is, keyof, let, module, namespace, never, new, null, number, object, of, package, private, protected, public, readonly, require, global, return, set, static, string, super, switch, symbol, this, throw, true, try, type, typeof, undefined, unique, unknown, var, void, while, with, yield, async, await},
  keywordstyle=\color{keywordcolor}\bfseries,
  ndkeywords={string, number, boolean, Promise, any, void},
  ndkeywordstyle=\color{blue}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{commentcolor}\ttfamily,
  stringstyle=\color{stringcolor}\ttfamily,
  morestring=[b]',
  morestring=[b]",
}

\lstset{
  language=TypeScript,
  backgroundcolor=\color{backgroundcolor},
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  frame=single,
}
\lstdefinelanguage{EnvCustom}{
  keywords={VITE_APP_API_URL, DATABASE_URL, DB_HOST, DB_USER, DB_PASSWORD, DB_PORT, DB_NAME, JWT_SECRET},
  keywordstyle=\color{blue}\bfseries,
  sensitive=true,
  comment=[l]{\#},
  commentstyle=\color{gray}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  stringstyle=\color{red}\ttfamily,
  identifierstyle=\color{black}
}

\lstdefinelanguage{JavaScriptCustom}{
  keywords={
    break, case, catch, class, const, continue, debugger, default, delete, do, else, export,
    extends, finally, for, function, if, import, in, instanceof, let, new, return, super, 
    switch, this, throw, try, typeof, var, void, while, with, yield, await, async,
    mysql, dotenv, process, config, createPool, getConnection, release
  },
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={true, false, null, undefined, NaN, Infinity},
  ndkeywordstyle=\color{teal}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{gray}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}


\lstdefinelanguage{SQL}
{
  keywords={SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER, BY, CREATE, PROCEDURE, FUNCTION, TRIGGER, BEGIN, END, DECLARE, IF, ELSE, RETURN, SET, INSERT, UPDATE, DELETE, INTO, VALUES, SIGNAL, JOIN, ON, AND, IN, GROUP, AS, RETURNS, DETERMINISTIC, COUNT, IFNULL, SQL, DATA, READS, THEN, AFTER, FOR, EACH, ROW, OLD, NEW, ROUND, CONSTRAINT, FOREIGN, KEY, REFERENCES, PRIMARY, CHECK, TABLE, DEFAULT, DESC, UNION, ALL, OVER, WHILE, DO, LEAVE, EXISTS, BEFORE, OR, IDENTIFIED, GRANT, TO, LEFT, RIGHT},
  keywordstyle=\color{blue}\bfseries,
  morekeywords=[2]{INT, CHAR, VARCHAR, BOOLEAN, DATE, DECIMAL, ENUM, TIME, CASCADE, NULL, NOT, USER, PRIVILEGES, TRUE},
  keywordstyle=[2]\color{red}\bfseries,
  comment=[l]{--},
  commentstyle=\color{gray}\ttfamily,
  morestring=[b]',
  stringstyle=\color{teal},
  basicstyle=\ttfamily\small,
  showstringspaces=false,
  breaklines=true
}

\lstset
{
  language=SQL,
  frame=single,
  breaklines=true,
  columns=flexible,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  backgroundcolor=\color{white}
}
\newcommand{\sql}[1]
{
  \begin{lstlisting}[language=sql]
    #1
  \end{lstlisting}
}
\newcommand{\python}[1]
{
  \begin{lstlisting}[language=python]
    #1
  \end{lstlisting}
}
\AtBeginDocument{\counterwithin{lstlisting}{section}}

\begin{document}
\fancyfoot{}
\coverpage\clearpage

\tableofcontents\clearpage
\listoffigures\clearpage

\setcounter{page}{1}
\fancyfoot[L]{\scriptsize \ttfamily Machine Learning Assignment Report\\1$^{\texttt{st}}$ Semester -- Academic year 2025-2026}
\fancyfoot[R]{\scriptsize \ttfamily Page {\thepage}/\pageref{LastPage}}

\section{Introduction and Project Objectives}

\subsection*{Aim of the Project}

The primary aim of this project is to develop and evaluate a machine learning model for predicting student academic outcomes - specifically, whether a student will graduate, remain enrolled, or drop out - using the \href{https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success}{\textit{Predict Students' Dropout and Academic Success}} dataset from the UCI Machine Learning Repository.\\

This assignment, part of an introductory Machine Learning course, focuses on applying supervised classification techniques to real-world educational data, emphasizing ethical considerations such as avoiding target leakage and handling class imbalance. By building predictive models, the project seeks to demonstrate how data-driven insights can support early interventions in higher education to improve retention rates and student success.

\subsection*{Specific Objectives}

\begin{enumerate}
  \item \textbf{Data Exploration and Preprocessing:} Analyze the dataset's features (demographic, socioeconomic, macroeconomic, academic) to understand distributions, correlations, and potential biases. Perform preprocessing, including feature engineering while ensuring no inclusion of leaky features from post-enrollment periods.

  \item \textbf{Feature Selection and Leakage Mitigation:} Identify and exclude features that introduce target leakage (e.g., second-semester academic metrics), focusing on enrollment-time and first-semester data for fair, prospective predictions.

  \item \textbf{Model Development:} Implement baseline and tuned classification models using Random Forest and Gradient Boosting algorithms, incorporating techniques like stratified splitting, robust scaling, one-hot encoding, and class weighting to address multiclass imbalance.

  \item \textbf{Hyperparameter Tuning and Evaluation:} Use randomized search with cross-validation to optimize model hyperparameters, evaluating performance through metrics such as accuracy, macro F1-score, precision, recall, and ROC-AUC. Assess overfitting and compare results against benchmarks to validate model robustness.

  \item \textbf{Interpretation and Insights:} Analyze feature importances to uncover key predictors of student outcomes, providing actionable recommendations for educational stakeholders.
\end{enumerate}

\subsubsection*{Primary Goal}

The primary goal is to predict student outcomes as a three-class classification problem: Graduate (completed the degree on time), Enrolled (still ongoing at the end of normal duration), or Dropout (left the institution). This supports early intervention strategies to reduce dropout rates. The dataset has no missing values, and preprocessing was performed to handle anomalies and outliers. Through these objectives, the project not only applies core machine learning concepts such as pipelines, tuning, and evaluation, but also highlights the importance of ethical modeling in sensitive domains like education, where biased or leaky predictions could mislead interventions.

\section{Exploratory Data Analysis}
\subsection{Data Loading and Initial Inspection}
The dataset was loaded from a CSV file (\texttt{data.csv}) into a Pandas DataFrame for analysis. Initial inspection revealed the following key details:

\begin{itemize}[parsep=0pt, itemsep=0pt, topsep=0pt]
  \item \textbf{Shape:} The dataset consists of \textbf{4424} rows (instances) and \textbf{37} columns (36 features + 1 target variable). For more details about each features' information please access \href{https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success}{\textit{UCI Machine Learning Repository}}.
  \item \textbf{Target Variable:} \texttt{Target} is the multiclass label with three categories:\\
        \begin{minipage}{0.48\textwidth}
          \begin{itemize}
            \item \textbf{Graduate}: 2209 instances ($\sim$50\%)
            \item \textbf{Dropout}: 1421 instances ($\sim$32\%)
            \item \textbf{Enrolled}: 794 instances ($\sim$18\%)
          \end{itemize}
          \textit{This indicates a moderate class imbalance, with Graduates as the majority class.}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
          \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{../images/target_distribution.png}
            \caption{Target Distribution}
            \label{target-distribution}
          \end{figure}
        \end{minipage}
  \item \textbf{Feature Types:}
        \begin{itemize}
          \item \textbf{Numerical (continuous/discrete):} 20 features (e.g., grades, ages, economic indicators)
          \item \textbf{Categorical (nominal/ordinal/binary):} 16 features (e.g., marital status, course, gender)
        \end{itemize}
  \item \texttt{Missing Values:} \textit{No missing values} were detected across the dataset.
  \item \texttt{Duplicates:} \textit{No duplicate rows} were found.
  \item \texttt{Data Types:} Mostly integers (discrete/binary/ordinal) and floats (grades/rates); the target is categorical (string).
\end{itemize}

A summary of some basic statistics for numerical features (mean, std, min, max) is provided below (computed via \texttt{df.describe()}):

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/numeric_describe.png}
  \caption{Numerical Features statistics}
  \label{num-describe}
\end{figure}


\paragraph{Data Insights and Summary Statistics}

\begin{itemize}[label=--]
  \item \textbf{Age Distribution:} Student ages range from 17 to 70 years, with a mean of approximately 23 years. This reflects a student body that is primarily comprised of traditional undergraduates, but also includes a non-negligible number of older, non-traditional students.
  \item \textbf{Grades:} Key academic grades, including admission and semester averages, fall within a 0-20 scale. The mean values generally range from 10 to 13, indicating that most students perform at or slightly above average, with substantial variability across the cohort.
  \item \textbf{Economic Indicators:} Features such as unemployment rate, inflation, and GDP are included to capture macroeconomic context for each student's enrollment year (2008-2019). Statistical summaries show these indicators vary only moderately across the sample, highlighting changing economic conditions that may indirectly influence student outcomes.
  \item \textbf{Semester Metrics:}
        \begin{itemize}[parsep=0pt, itemsep=0pt, topsep=0pt, label=+]
          \item \textit{Approvals/Evaluations:} Metrics for the first semester show higher rates of approved units and evaluations when compared to the second semester.
          \item \textit{Dropout Effect:} This discrepancy is likely explained by student attrition—some students drop out after the first semester, leading to incomplete or reduced second-semester data.
        \end{itemize}
\end{itemize}

These insights inform both preprocessing and model development by clarifying typical data ranges, potential sources of variance, and the importance of handling non-standard cases such as older students or those affected by economic conditions.

\subsection{Data Visualization and Insights}

Visualizations were generated using \texttt{Matplotlib} and \texttt{Seaborn} to explore distributions, relationships, and patterns. All figures were saved to the images directory for inclusion in the report. Key visualizations and insights are described below.

\subsubsection{Numerical Feature Distributions}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/numeric_histograms.png}
  \caption{Numerical Feature Distributions}
  \label{num_dist_his}
\end{figure}

\paragraph*{Insights}

The histograms with kernel density estimates (KDE) for selected numerical features highlight distributional characteristics that guide preprocessing steps such as transformation, outlier handling, and scaling to improve model performance and stability:
\begin{itemize}
  \item \textbf{Previous Qualification (grade):} The distribution is roughly symmetric around 130 but shows multimodality with spikes (e.g., at 125, 140), possibly due to grading scales or binning effects. Range 95--190 with std $\approx$13.2.
  \item \textbf{Admission grade:} Multimodal with peaks around 120--140, similar variance (std $\approx$14.5) and range (95--190). Indicates clustered admission scores. Robust scaling to mitigate peak influences in distance metrics like KNN.
  \item \textbf{Enrollment Age:} Heavily right-skewed (mean 23.3, median 20) with long tail up to 70, most mass at 18--25. Potential outliers beyond 40. Group ages to reduce parsity
  \item \textbf{1st Semester Approved Units:} Right-skewed with mode at 5--6, many low values (25\% at 3, min 0), max 26. Zero-inflated aspect for non-approvals. Treat zeros separately (e.g., binary flag for zero approvals) to handle excess zeros in count-based models.
  \item \textbf{2nd Semester Approved Units:} Similar to 1st semester but slightly shifted (mean 4.4, more spread in low approvals). Right-skewed with potential zeros. Aggregate with 1st semester if correlated; apply square-root transform for variance stabilization in Poisson-like distributions would be a solid option.
  \item \textbf{2nd Semester Grade:} Bimodal with peaks at 0--5 (failures?) and 10--15 (passes), mean 10.2, std 5.2. Heavy mass at low ends, indicating grade inflation or dropout effects. Cap or bin grades into categories if granularity adds noise, especially for tree-based models tolerant to non-normality.
\end{itemize}

In summary, skewed features like age and approvals benefit from transformations (log/sqrt) to approximate normality for parametric models, while grades may require robust scaling due to multimodality. Outlier clipping and zero-handling are crucial for academic metrics to avoid bias in predictions of student success.

\subsubsection{Key Categorical Features' Distributions}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/target_by_categorical_all_count.png}
  \caption{Key Categorical Distribution}
  \label{cat_dist_dist}
\end{figure}
\paragraph{Insights}
\begin{itemize}
  \item \textbf{Marital Status:} Almost all students are single (category 1), with very few in other marital states. This feature exhibits severe imbalance and limited variation, suggesting it may have minimal predictive power or could be grouped into binary categories (single vs. others) to reduce sparsity.
  \item \textbf{Application Mode:} The large concentration in application mode 1 with several smaller categories indicates a high cardinality categorical variable with skewed representation. Rare categories might be grouped or encoded carefully to avoid noise and sparsity in the model.
  \item \textbf{Course:} The counts are spread across many courses, with one course having a notably high count. This high dimensionality and imbalance may require dimensionality reduction, grouping of less common courses, or using embedding techniques for encoding.
  \item \textbf{Daytime/Evening Attendance:} Shows a clear majority in daytime attendance (category 1), but both categories have sufficient representation. This categorical feature is well balanced for direct encoding.
  \item \textbf{Special Needs:} Overwhelming majority do not have special needs, making this feature highly imbalanced. It may have limited impact unless special needs cases strongly associate with outcomes, warranting inclusion with careful encoding or as a binary flag.
  \item \textbf{Debtor:} Shows class imbalance with most students not being debtors. This binary feature can be used as is, but attention should be paid during model evaluation to account for imbalance effects.
  \item \textbf{Scholarship:} A similar pattern to debtor status, with most students not receiving scholarships. While there is imbalance, the feature may add useful information on socioeconomic status if encoded suitably.
  \item \textbf{Tuition Fees Up to Date:} Nearly all students have tuition fees up to date except for a significant minority. This binary feature appears informative and well suited for direct inclusion.
\end{itemize}

In summary, categorical features with many rare categories (Application Mode, Course, Marital Status) will benefit from grouping or specialized encoding to reduce sparsity. Features with binary outcomes and moderate imbalance can be used directly with attention to balancing during model training.

\subsubsection{Correlation Heatmap}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/correlation_heatmap.png}
  \caption{Correlation Heatmap}
  \label{core_heat}
\end{figure}
\paragraph{Insights}
\begin{itemize}
  \item \textbf{1st and 2nd Semester Academic Metrics:} These show strong positive correlations within and across semesters (often $>$ 0.7, e.g., 1st approved vs. 2nd approved $\approx$ 0.90). This indicates redundancy; preprocessing should involve aggregating (e.g., total approvals) or applying PCA to reduce dimensions and avoid multicollinearity in linear models.
  \item \textbf{Parental Qualifications and Occupations (Mother's/Father's Qualification, Occupation):} Moderate correlations (0.4--0.7) suggest socioeconomic clustering. Combine into a parental background score via averaging or factor analysis to simplify without losing information.
  \item \textbf{Macroeconomic Indicators (Unemployment rate, Inflation rate, GDP):} Very weak correlations with all features ($<$ 0.1 absolute), implying low relevance. Drop these during feature selection to minimize noise and model complexity.
  \item \textbf{Enrollment Age:} Low correlations overall ($<$ 0.3) but potential for outliers (range 17--70). Apply winsorization to handle skewness and extreme values that could distort distance-based algorithms.
  \item \textbf{Admission and Previous Grades (Admission grade, Previous qualification grade):} Moderate links to semester grades (0.3--0.5), indicating some predictive value. Standardize scales as they differ from semester grades; retain but monitor for multicollinearity with academic outcomes.
  \item \textbf{International and Nationality:} High correlation ($\approx$ 0.9), as one derives from the other. CLearly we should remove one to eliminate perfect collinearity issues.
  \item \textbf{Gender and Scholarship:} Weak correlations ($<$ 0.2) but binary nature makes them easy to encode. Retain with one-hot or label encoding; address any class imbalance via sampling if linked to target in further analysis.
  \item \textbf{Displaced and Special Needs:} Low inter-correlations ($<$ 0.1); treat as flags. Binary encoding suffices, but check distributions for rarity and potential merging if sparse.
\end{itemize}

In summary, focus on reducing redundancy in highly correlated academic and family features through aggregation or dimensionality reduction, while dropping weakly correlated macroeconomic variables. Scaling is essential for features with varying ranges (e.g., age, grades) to ensure compatibility in machine learning pipelines.

\section{Data Preprocessing and Feature Preparation}
\subsection{Anomaly Detection and Cleaning}
Although the dataset is reported to contain no missing (NaN) values, we nonetheless perform thorough data cleaning as follows to ensure integrity and consistency:
\begin{lstlisting}[language=python]
# Drop invalid ages (enroll age <17 or >70, or application order <0)
df = df[(df['Enroll Age'] >= 17) & (df['Enroll Age'] <= 70)]
df = df[df['Application order'] >= 0]
# Drop invalid grades (admission or previous <95 or >190)
df = df[(df['Admission grade'] >= 95) & (df['Admission grade'] <= 190)]
df = df[(df['Pre Qual (grade)'] >= 95) & (df['Pre Qual (grade)'] <= 190)]
# Drop invalid semester metrics (e.g., approved > enrolled, evaluations <0)
for sem in ['1st', '2nd']:
    df = df[df[f'{sem} - approved'] <= df[f'{sem} - enrolled']]
    df = df[df[f'{sem} - evaluations'] >= 0]
    df = df[df[f'{sem} - grade'] >= 0]
# Drop rows with invalid binary/ordinal values (e.g., gender not 0/1)
df = df[df['Gender'].isin([0, 1])]
df = df[df['Scholarship'].isin([0, 1])]
df = df[df['Tuition fees up to date'].isin([0, 1])]
\end{lstlisting}

\subsection{Grouping of Categorical Features}
In the preprocessing pipeline, several categorical features with high cardinality or sparse codes were grouped into broader, more interpretable categories to reduce dimensionality, mitigate overfitting, and enhance model performance.
\begin{lstlisting}[language=python]
  df_prep = df_prep.drop(columns=['Debtor', 'Special Needs', 'Unemployment rate', 'Inflation rate', 'GDP'])
  col_func_map = {
      'Marial Status': lambda x: marial(x),
      'Application mode': lambda x: app_mode(x),
      'Course': lambda x: course(x),
      'Pre Qual': lambda x: pre_qual(x),
      'Nationality': lambda x: nationality(x),
      "Mom's Qual": lambda x: qual(x),
      "Dad's Qual": lambda x: qual(x),
      "Mom's Occupation": lambda x: moms_job(x),
      "Dad's Occupation": lambda x: dads_job(x)
  }
  for col, func in col_func_map.items():
      df_prep[col] = df_prep[col].apply(func)
\end{lstlisting}
Grouping decisions were informed by domain knowledge and the dataset documentation, where feature codes correspond to specific educational, occupational, or administrative groups. The motivations for this step include:
\begin{itemize}
  \item \textbf{Dimensionality Reduction:} Categorical features like Application mode or Course would produce sparse and high-dimensional encodings if left ungrouped. Grouping codes with similar meaning reduces this sparsity and enhances the stability of subsequent models.
  \item \textbf{Interpretability:} Aggregating codes along logical groupings (e.g., collapsing educational qualifications into ``basic'', ``secondary'', and ``higher'' categories) results in features that are easier to interpret and reason with, both in exploratory analysis and model output.
  \item \textbf{Handling Imbalance:} Many categorical codes have very few samples merging these rare categories prevents poorly-represented classes from introducing noise or overfitting.
  \item \textbf{Leakage Avoidance:} All groupings were defined based only on code descriptions and situational knowledge, and were applied prior to any exposure to the outcome labels. Thus, target leakage is avoided.
\end{itemize}
This process ensures the resulting categorical variables are meaningful, compact, and suitable for downstream encoding and analysis (as shown in \textcolor{red}{preprocess.ipynb}):

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{../images/target_by_categorical_preprocessed.png}
  \caption{Grouped Categorical Features}
  \label{grouped_cat}
\end{figure}

Post-grouping plots reveals consistent patterns: Socioeconomic advantages (higher parental education/occupation, scholarships) boost graduation, while non-standard admissions or manual backgrounds increase dropout risk. Imbalances persist (e.g., dominant groups skew distributions), necessitating stratified sampling and weighted metrics. Grouping reduced features from high-cardinality to 2-7 levels, improving efficiency (e.g., OHE dimensions from around 100 to around 30). In modeling, these features ranked moderately in importances, interacting with academics. Overall, this step enhanced predictive power and interpretability, enabling targeted educational strategies.

\section{Baseline Model Training and Evaluation}
The training process utilized the preprocessed and engineered dataset to build and evaluate multiclass classification models for predicting student outcomes. The workflow included data splitting, pipeline construction, baseline training, hyperparameter tuning via randomized search, comprehensive evaluation, overfitting checks, feature importance analysis, and visualization of confusion matrices. All steps emphasized imbalance mitigation (stratified splits, class weights) and reproducibility (\texttt{random\_state}~$=42$).
\begin{lstlisting}[language=python]
preprocessor = ColumnTransformer(
  transformers=[
      ('num', RobustScaler(), numeric_cols),
      ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)],
  remainder='drop'
)
\end{lstlisting}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{../pics/pipeline_pretrain.png}
  \caption{Pretrain Pipeline}
  \label{pretrain}
\end{figure}

\subsection{Model Selection}
The dataset contains $4424$ instances with $36$ features, including numerical, categorical, and discrete integers, aimed at classifying students into dropout, enrolled, or graduate categories. It has no missing values but features strong class imbalance, making it ideal for educational predictive modeling to identify at-risk students early through machine learning classifiers. Focus was on tree-based ensembles — Random Forest and Gradient Boosting - due to their handling of mixed features and interpretability.
\begin{lstlisting}[language=python]
models = {
    'RandomForest': RandomForestClassifier(n_estimators=400, max_depth=None, random_state=42, class_weight='balanced_subsample', n_jobs=-1),
    'GradientBoosting': GradientBoostingClassifier(random_state=42)
}
\end{lstlisting}
\subsubsection*{Random Forest Classifier}
Random Forest works by creating an ensemble of multiple decision trees, each trained on a random subset of the data (bootstrapping) and features, then aggregating their predictions through majority voting for classification or averaging for regression. This reduces overfitting, handles variance well, and improves accuracy by leveraging diversity among the trees.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../pics/rf.png}
  \caption{Random Forest Classifier}
  \label{rf_clf}
\end{figure}
Random Forest (RF) is suitable for this dataset as an ensemble method that builds multiple decision trees via bagging, effectively handling mixed feature types, non-linear relationships, and class imbalance with techniques like class weighting. It reduces overfitting through randomness and provides feature importance scores, offering interpretable insights into factors like socio-economic status or grades that influence dropout, while achieving robust performance on tabular data without extensive preprocessing.
\subsubsection*{Gradient Boosting Classifier}
Gradient Boosting builds trees sequentially, where each new tree corrects the errors of the previous ones by minimizing a loss function using gradient descent. It focuses on hard-to-predict instances, often achieving high performance through additive modeling, with regularization to prevent overfitting.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../pics/gb.png}
  \caption{Gradient Boosting Classifier}
  \label{gb_clf}
\end{figure}
Gradient Boosting excels by sequentially building trees to correct errors, capturing complex interactions and subtle patterns in the data better than single models. It addresses imbalance with built-in weighting and regularization to prevent overfitting, often yielding higher accuracy on imbalanced educational datasets, and includes feature importance for explaining predictions, making it a strong choice for model selection to optimize metrics like F1-score in student success forecasting.
\subsection{Model Training and Performance Evaluation}
\subsubsection*{Training Process}
\begin{lstlisting}[language=python]
for name, model in models.items():
  pipe = Pipeline([
      ('prep', preprocessor),
      ('model', model)
  ])
  pipe.fit(X_train, y_train)
  y_pred = pipe.predict(X_test)
\end{lstlisting}
\subsubsection*{Evaluation Metrics and Insights}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/precision_recall_curve.png}
  \caption{Precision - Recall Curve}
  \label{pr_curve}
\end{figure}
Overall, both models demonstrate the typical precision-recall tradeoff, with precision increasing and recall decreasing as the decision threshold rises. Gradient Boosting curves are smoother and exhibit steeper transitions, especially for the Enrolled class, indicating better probability calibration and sharper discrimination across thresholds. The Random Forest curves, while broadly following the same trend, show more variability—especially minor fluctuations in recall for the Enrolled group—likely a reflection of the model's inherent randomness due to bagging.\\

For the majority class (Graduate), both precision and recall remain consistently high and stable, as expected due to class imbalance favoring this group. In contrast, the minority class (Enrolled) presents erratic, lower precision and recall curves, highlighting difficulty in separating this group from others - a common challenge in imbalanced datasets, even when using class weights. Notably, GB attains slightly higher precision at mid-range thresholds, particularly for Enrolled, and this aligns with its marginally superior F1\_macro score. This advantage suggests that \textit{GB more effectively balances} precision and recall for underrepresented classes, reaffirming its selection for optimizing imbalanced multi-class problems.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/eval.png}
  \includegraphics[width=\textwidth]{../images/confusion_matrix.png}
  \caption{Some Evaluation Metrics}
  \label{eval_metrics}
\end{figure}
\paragraph*{Key Insights:}
\begin{itemize}
  \item \textbf{Gradient Boosting} excels in probability calibration and precision/recall control, making it especially suitable for imbalanced multiclass problems where identifying and supporting minority classes is vital. Its consistently strong macro-averaged F1-score provides additional support for its selection in such contexts.
  \item \textbf{Random Forest} - while somewhat less precise with calibrated probabilities - provides robustness to noise and outliers, making it valuable when input features are heterogeneous or noisy.
  \item Both models, particularly when complemented by customized threshold selection and interpretability tools, enable actionable insights for institutions: facilitating risk stratification, optimally directing resources, and improving support for students most at risk of adverse outcomes.
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/top_10_feature_importances_overlap.png}
  \caption{Top 10 Feature Importances}
  \label{top10_features}
\end{figure}

Both models highlight academic performance in the early semesters as key predictors of student outcomes, aligning with intuitive expectations: early success (or failure) often signals overall trajectory in higher education. However, the models differ significantly in how they distribute importance, reflecting their algorithmic differences - Random Forest (an ensemble of independent trees) tends to spread importance more evenly, while Gradient Boosting (sequential error-correcting trees) can concentrate on a few dominant features.
\begin{itemize}
  \item \textbf{Gradient Boosting:} High concentration on one feature indicates potential efficiency but risks over-reliance if this feature is noisy or correlated. This makes Gradient Boosting sensitive to second-semester data.
  \item \textbf{Random Forest:} The even distribution reflects it's bagging approach, averaging across diverse trees to reduce variance. This makes it less prone to overemphasizing one feature, potentially leading to more generalizable insights.
\end{itemize}

\section{Model Enhancement and Optimization}
\subsection{Feature Engineering}
To enhance the predictive power of the dataset, a custom function was applied to derive new variables from raw academic and demographic data. This step focused on creating relative metrics that normalize absolute values, making them more comparable across students and capturing trends in performance. These engineered features proved highly informative for modeling, as indicated by their consistently high rankings in feature importance analyses, outperforming raw counts by emphasizing success efficiency and academic momentum - critical signals for identifying dropout risk.
\begin{lstlisting}[language=python]
def create_engineered_features(df):
    # Semester-level rates and averages
    for sem in ['1st', '2nd']:
        enrolled = df[f'{sem} - enrolled']
        approved = df[f'{sem} - approved']
        evaluations = df[f'{sem} - evaluations']
        grade = df[f'{sem} - grade']
        # Approval rate: approved / enrolled (zero-denominator safe)
        df[f'{sem}_approval_rate'] = approved.divide(enrolled.replace(0, np.nan)).fillna(0)
        # Evaluation rate: evaluations / enrolled
        df[f'{sem}_evaluation_rate'] = evaluations.divide(enrolled.replace(0, np.nan)).fillna(0)
        # Average grade: grade / evaluations
        df[f'{sem}_avg_grade'] = grade.divide(evaluations.replace(0, np.nan)).fillna(0)
    # Performance improvement deltas (2nd - 1st; for analysis)
    df['delta_approval_rate'] = df['2nd_approval_rate'] - df['1st_approval_rate']
    df['delta_avg_grade'] = df['2nd_avg_grade'] - df['1st_avg_grade']
    # Age binning: four categories (0: <=20, 1: 21-24, 2: 25-30, 3: >30)
    df['AgeGroup'] = pd.cut(
        df['Enroll Age'],
        bins=[-1, 20, 24, 30, np.inf],
        labels=[0, 1, 2, 3]
    ).astype(int)
    return df
\end{lstlisting}

The main derived features are:
\begin{itemize}
  \item \textbf{Semester-Level Rates:} For each semester, \texttt{approval\_rate}, \texttt{evaluation\_rate}, and \texttt{avg\_grade} were computed with safeguards for division by zero. These normalized ratios deliver robust insights into academic engagement and success, better reflecting student trajectories than absolute counts.
  \item \textbf{Improvement Deltas:} Differences between second and first semester rates were calculated to quantify progress or decline. While these helped analyze performance changes, they were omitted from final models to prevent introduction of future-data leakage.
  \item \textbf{Age Grouping:} The continuous \texttt{Enroll Age} variable was binned into four interpretable categories (0: $\leq$ 20, 1: 21--24, 2: 25--30, 3: $>$ 30) to reduce noise and help models detect age-related risk factors.
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/eng_feats.png}
  \caption{New Features Distribution}
  \label{eng_feats}
\end{figure}

These visualizations reveal distinct distributional patterns that underscore the features' value in distinguishing outcomes. High approval/grade rates strongly favor Graduates, while negative deltas and older age groups signal dropout risk. These insights validate the engineering choices, as the new features capture normalized performance and trends more effectively than raw metrics, ranking highly in model importances and improving predictive separation without introducing bias. These enhancements improved both model interpretability and predictive accuracy by emphasizing relative academic performance, validated by the dominance of rate-based features in the model's importance rankings.
\subsection{Evaluation of Retrained Models}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/precision_recall_curve_enhanced.png}
  \caption{Enhanced Models' Precision - Recall Curve}
  \label{pr_curve_enhanced}
\end{figure}

Feature engineering and hyperparameter tuning have clearly enhanced model performance. The post-tuning curves show improved precision, more stable recall, and better overall trade-offs -- especially for the previously unstable Enrolled class. These improvements indicate more confident and reliable model predictions across all classes. Comparing models, Gradient Boosting generally shows smoother precision-recall trends than Random Forest, especially for the Graduate and Dropout classes. It also tends to achieve slightly higher precision at higher thresholds. However, both models struggle similarly with the Enrolled class, suggesting the issue lies more with the data than the model itself.\\

After retraining with the engineered features and tuned hyperparameters, both models showed improved performance, especially in handling the class imbalance:
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../pics/metrics_enhanced.png}
  \includegraphics[width=\textwidth]{../images/confusion_matrix_enhanced.png}
  \caption{Enhanced Models' Metrics}
  \label{metrics_enhanced}
\end{figure}
While the accuracy lift was modest, the macro F1-score and ROC-AUC scores improved, indicating a better balance in predicting the minority classes. The new feature importances confirmed the value of feature engineering, with \texttt{2nd\_approval\_rate} becoming the top predictor for the tuned Random Forest model. An overfitting analysis showed the difference between cross-validation and test scores was minimal and acceptable for both models.

\subsection{Hyperparameter Optimization}
\textbf{Set up}
\begin{itemize}[itemsep=0pt, topsep=0pt, label=---]
  \item A custom scoring function using macro-averaged F1 (\texttt{f1\_scorer}) is created. This metric gives equal weight to each class, making it well-suited for imbalanced problems where accuracy alone could be misleading.
  \item Cross-validation is configured using \texttt{StratifiedKFold} (\texttt{cv}) with 5 splits and shuffling. This ensures the proportion of each class is maintained in all folds, preventing bias during training and model evaluation.
  \item Class-balanced sample weights (\texttt{sample\_weights}) are computed for \texttt{y\_train} using sklearn's 'balanced' strategy. These weights can be supplied to models so that each class has an appropriate influence in the loss, offsetting the effects of class imbalance and helping avoid bias toward the majority class.
\end{itemize}
\begin{lstlisting}[language=python]
# Custom scorer for imbalanced data
f1_scorer = make_scorer(f1_score, average='macro')
# Setup cross-validation with proper stratification
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
sample_weights = compute_sample_weight('balanced', y_train);
\end{lstlisting}

\subsubsection{Random Forest}
To optimize the Random Forest model, a hyperparameter tuning pipeline was constructed using Scikit-learn's \textit{Pipeline} and \textit{RandomizedSearchCV}. This approach ensures that preprocessing steps are properly applied within each cross-validation fold, preventing data leakage and leading to a more reliable evaluation of the model's performance.
\subsubsection*{Tuning Pipeline}
\begin{lstlisting}[language=python]
# Parameter grids
rf_param_grid = {
  'n_estimators': [300, 400, 500],
  'max_depth': [20, 25, 30],
  'min_samples_split': [5, 10, 15],
  'min_samples_leaf': [2, 4, 6],
  'max_features': ['sqrt', 'log2'],
  'class_weight': ['balanced_subsample'],
  'criterion': ['gini', 'entropy']
}
# Create pipelines outside of search
def create_rf_pipeline():
    return Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))
    ])
\end{lstlisting}
\subsubsection*{Hyperparameter Tuning Process}
\begin{lstlisting}[language=python]
rf_search = RandomizedSearchCV(
  create_rf_pipeline(),
  param_distributions={'classifier__' + k: v for k, v in rf_param_grid.items()},
  n_iter=50,
  scoring={'f1_macro': f1_scorer, 'accuracy': 'accuracy'}, refit='accuracy',
  cv=cv,
  random_state=42,
  n_jobs=-1,
  verbose=0,
  return_train_score=True
)

rf_search.fit(X_train, y_train)
\end{lstlisting}
\subsubsection{Gradient Boosting}
\subsubsection*{Tuning Pipeline}
To optimize the Gradient Boosting model, a tuning pipeline was created using Scikit-learn's \texttt{Pipeline}. The pipeline first applies the \texttt{preprocessor} to handle feature scaling and encoding, ensuring that all preprocessing steps are performed within each cross-validation fold. The preprocessed data is then fed into the \texttt{GradientBoostingClassifier}.\\

A parameter grid, \texttt{gb\_param\_grid}, was constructed to guarantee that transformation and modeling steps are coordinated and prevents data leakage. By integrating the pipeline with \texttt{RandomizedSearchCV}, the hyperparameter search yields a robust and well-calibrated model.
\begin{lstlisting}[language=python]
# Parameter grids
gb_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.15],
    'max_depth': [3, 5, 7, 9],
    'min_samples_split': [10, 20, 30],
    'min_samples_leaf': [5, 10, 15],
    'subsample': [0.8, 0.9, 1.0],
}
# Create pipelines outside of search
def create_gb_pipeline():
    return Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', GradientBoostingClassifier(random_state=42))
    ])
\end{lstlisting}
\subsubsection*{Hyperparameter Tuning Process}
\begin{lstlisting}[language=python]
gb_search = RandomizedSearchCV(
  create_gb_pipeline(),
  param_distributions={'classifier__' + k: v for k, v in gb_param_grid.items()},
  n_iter=50,
  scoring={'f1_macro': f1_scorer, 'accuracy': 'accuracy'}, refit='accuracy',
  cv=cv,
  random_state=42,
  n_jobs=-1,
  verbose=0,
  return_train_score=True
)

gb_search.fit(X_train, y_train, classifier__sample_weight=sample_weights)
\end{lstlisting}
\subsection{Performance Metrics and Model Interpretation}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/confusion_matrix_hypetuned.png}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../pics/metrics_hypertuned.png}
  \caption{Hypertuned Models' Metrics}
\end{figure}
The final hypertuned models shift focus from maximizing raw accuracy to achieving balanced performance, particularly improving classification of minority classes in the student dropout prediction task:
\begin{itemize}
  \item \textbf{Random Forest (Hypertuned)}:
        \begin{itemize}[parsep=0pt, itemsep=0pt, topsep=0pt]
          \item Accuracy decreases slightly to 0.763 (from engineered 0.781 and naive 0.769).
          \item \textbf{Macro F1} improves to 0.718 (\textbf{+3\%} vs naive).
          \item \textbf{Recall} rises to 0.720 (\textbf{+5\%} vs naive).
          \item \textbf{ROC-AUC} increases to 0.887 (+1\%), showing improved discrimination capacity.
          \item Notably, minority class (\textit{Enrolled}) recall increases, with underprediction reduced by 50\% compared to naive, highlighting better handling of class imbalance.
        \end{itemize}
  \item \textbf{Gradient Boosting (Hypertuned)}:
        \begin{itemize}[parsep=0pt, itemsep=0pt, topsep=0pt]
          \item Accuracy falls to 0.768 (from naive 0.783 and engineered 0.778).
          \item Macro F1 steadies at 0.712 after prior drop.
          \item Recall holds at $\sim$0.708, matching engineered.
          \item ROC-AUC remains strong at 0.882.
          \item Improvements primarily manifest as reduced overfitting and more balanced generalization rather than direct metric gains.
        \end{itemize}
\end{itemize}

\textbf{Summary:} Hypertuning sacrifices about 1--2\% accuracy to deliver higher macro metrics, fairer class-wise performance, and more reliable \textit{Enrolled} (minority) prediction. In imbalanced educational datasets, this approach is preferable over naive or merely feature-engineered models that optimize only for accuracy.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/top10_feature_importances_enhanced.png}
  \caption{Enhanced Models' Top 10 Feature Importances}
  \label{top10_enhanced}
\end{figure}
\subsubsection*{Insights:}

\textbf{Random Forest:} In the baseline, raw academic performance counts dominate feature importance, with \emph{2nd - approved} being the most impactful feature. After enhancement, engineered ratios such as approval rates and average grades take precedence, indicating that the model benefits from summarized metrics rather than raw counts. Additionally, feature importance becomes more balanced post-tuning, suggesting reduced overfitting and a more generalized model.\\

\textbf{Gradient Boosting:} The baseline model demonstrates extreme dominance by \emph{2nd - approved}, with this feature accounting for over 50\% of the total importance -- a sign of potential overfitting. Following feature engineering and hyperparameter tuning, its importance drops significantly. Instead, engineered features such as \emph{2nd\_approval\_rate} and \emph{2nd\_avg\_grade} emerge as key predictors. Admission-related attributes also gain prominence in the tuned model, leading to a more distributed and potentially robust set of important features.

\section{Ensemble Method}
\subsection{Voting Classifier Implementation}
Ensemble voting is a popular technique in ensemble learning where the final prediction is made by combining the predictions of multiple individual models. It is commonly used for classification tasks, although it can also be applied to regression problems. In this technique, each individual model in the ensemble independently makes predictions on the input data. The final prediction is determined by combining the predictions of these models using a voting scheme.

\begin{minipage}{0.4\textwidth}
  \begin{figure}[H]
    \centering
    \caption{Voting Classifier Pipeline}
    \includegraphics[width=\textwidth]{../pics/voting_pipeline.png}
    \label{pipe_vote}
  \end{figure}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
  \begin{lstlisting}[language=python]
ensemble = VotingClassifier(
  estimators=[
      ('rf', rf_search.best_estimator_),
      ('gb', gb_search.best_estimator_)
  ],
  voting='soft') # To be discussed
ensemble.fit(X_train, y_train)
  \end{lstlisting}
\end{minipage}

\textbf{Why soft voting?}
\begin{itemize}
  \item \textbf{Both base models support probabilities:} The ensemble combines a RandomForestClassifier and GradientBoostingClassifier, both of which provide predicted class probabilities via their \texttt{predict\_proba} methods. Soft voting leverages these probabilities by averaging them across models and selecting the class with the highest mean probability, rather than simply voting on class labels.
  \item \textbf{Multi-class problem with class imbalance:} The prediction task involves three distinct outcome classes (\emph{Dropout}, \emph{Enrolled}, \emph{Graduate}), with certain classes (such as \emph{Enrolled}) being underrepresented. Soft voting is advantageous here as it incorporates the confidence of each model's prediction, allowing the ensemble to account for subtle distinctions and avoid bias towards the majority class, unlike hard voting where ties or majority label counts may obscure minority classes.
  \item \textbf{Improved decision confidence:} By weighting class probabilities, soft voting enables more nuanced and informed consensus decisions, especially when individual models express uncertainty. This typically results in improved overall and minority class performance for imbalanced multi-class datasets such as this one.
\end{itemize}

\subsection{Final Models Comparison}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../pics/3models_comparison.png}
  \caption{Final Comparison}
  \label{final_comp}
\end{figure}

\subsubsection*{Key Insights}
\begin{itemize}
  \item \textbf{Overall Best Performer:} The Ensemble model consistently outperforms the individual models across most evaluation metrics, achieving the highest accuracy, F1\_Weighted (0.772), Precision\_Macro (0.726), and a strong ROC\_AUC (0.887). This demonstrates the effectiveness of combining Random Forest's balanced predictions with Gradient Boosting's depth via soft voting, which mitigates individual model weaknesses - such as the lower macro scores of Gradient Boosting.
  \item \textbf{Accuracy and Weighted Metrics:} While Gradient Boosting slightly outperforms Random Forest in accuracy (0.768 vs. 0.763), the Ensemble achieves an even higher score. Weighted F1 and Precision values, which emphasize performance on majority classes, are highest for the Ensemble, highlighting its robustness for imbalanced datasets.
  \item \textbf{Macro Metrics (Class Balance):} Random Forest leads in F1\_Macro (0.718 vs. 0.712) and Recall\_Macro (0.720 vs. 0.708) when compared to Gradient Boosting, signaling stronger detection of the minority class (Enrolled). The Ensemble matches Random Forest on F1\_Macro and surpasses it in macro precision, achieving a favorable balance across all classes without substantial sacrifices in recall.
\end{itemize}

\section{Conclusion}
This machine learning assignment project aimed to predict students' academic outcomes -- specifically, whether they would graduate, remain enrolled, or drop out -- using a comprehensive dataset encompassing demographic, socioeconomic, and academic performance attributes. By leveraging advanced modeling techniques, we successfully developed robust predictive models that achieved over 77\% accuracy, providing valuable insights into factors influencing student success and demonstrating the practical application of machine learning in educational analytics.

\subsection{Summary of Methodology}
\subsubsection*{Dataset Insights and Exploratory Data Analysis (EDA)}
The project began by developing a systematic and iterative workflow in line with machine learning best practices. In the initial \textbf{dataset insights} stage, we loaded and inspected the raw data to understand its structure. This was followed by extensive visualizations, including distributions of numerical features, breakdowns of key categorical variables, and a correlation heatmap. These analyses were used to identify data patterns, class imbalances, and relationships among critical variables such as enrollment age, tuition payment status, and semester-based performance metrics. This EDA phase revealed important trends -- for example, the presence of class imbalances in the target variable and strong correlations between academic approvals and outcomes -- providing a strong foundation for further analysis and decision-making.

\subsubsection*{Preprocessing}
The next phase, \textbf{preprocessing}, focused on ensuring data quality and preparing the dataset for modeling. This included handling missing values, encoding categorical features, scaling numerical variables, and splitting the data into training and testing subsets. These steps were critical to mitigate issues such as multicollinearity, providing a clean and robust pipeline suitable for downstream machine learning tasks.

\subsubsection*{Training Pipelines}
During the \textbf{training pipelines} step, we selected appropriate models for the task, concentrating on Random Forest and Gradient Boosting due to their proven ability to capture complex, non-linear relationships. Baseline models were trained, and initial evaluations were conducted using metrics such as accuracy, weighted and macro F1-score, precision, recall, and ROC-AUC. These results highlighted areas of solid performance as well as opportunities to address challenges related to class imbalance and potential overfitting.

\subsubsection*{Model Improvements and Tuning}
To further enhance predictive performance, we carried out \textbf{model improvements} via feature engineering. New attributes were constructed, such as approval rates (e.g., \texttt{2nd\_approval\_rate}), average grades (e.g., \texttt{2nd\_avg\_grade}), and performance deltas (e.g., \texttt{delta\_avg\_grade}). Retraining models on these engineered features generated noticeable improvements in balanced evaluation metrics. Further refinement was achieved through hyperparameter tuning using grid search for both Random Forest (e.g., \texttt{max\_depth}, \texttt{n\_estimators}) and Gradient Boosting (e.g., \texttt{learning\_rate}, \texttt{subsample}). Post-tuning analysis showed a shift in feature importance from raw counts (such as ``2nd - approved'' in baseline models) towards the newly engineered ratios---a change that reduced overfitting and improved overall model generalization, as reflected in F1\_Macro ($\sim$0.718) and ROC\_AUC ($\sim$0.887).

\subsubsection*{Ensemble Learning}
The final step, \textbf{ensemble learning}, combined the strengths of the individually tuned Random Forest and Gradient Boosting models through a soft-voting classifier. This method averaged predicted class probabilities to better leverage the confidence scores of each model. Particularly well-suited to our multi-class, imbalanced classification problem, this strategy produced the highest performance across all approaches: an accuracy of 0.776, F1\_Macro of 0.718, and ROC\_AUC of 0.887 -- consistently outperforming single models by 1--2\% and notably improving detection of the minority class (\emph{Enrolled}).

\subsection{Discussion, Limitations, and Future Work}
Key findings underscore the importance of academic performance indicators, such as second-semester approvals and grades, in predicting outcomes, with engineered features proving instrumental in model refinement. The ensemble's superior results highlight the value of combining complementary algorithms to mitigate individual limitations and achieve more reliable predictions.

In practical terms, this work lays the groundwork for deploying early warning systems in educational institutions, enabling proactive interventions like targeted counseling or resource allocation for at-risk students. Limitations include potential dataset-specific biases and the absence of real-time data; future work could incorporate additional models (e.g., neural networks), external features (e.g., socioeconomic trends), or deployment via web applications for broader impact. Overall, this project not only met its objectives but also exemplified a complete machine learning lifecycle, from exploration to optimization, contributing meaningfully to predictive analytics in education.

\begin{thebibliography}{9}
  \bibitem{scikit-learn}
  Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O. (2011). Scikit-learn: Machine learning in Python. \textit{Journal of Machine Learning Research}, 12, 2825-2830.

  \bibitem{ensemble}
  Rokach, L. (2010). Ensemble-based classifiers. \textit{Artificial Intelligence Review}, 33(1-2), 1-39.

  \bibitem{imbalanced}
  He, H., \& Garcia, E. A. (2009). Learning from imbalanced data. \textit{IEEE Transactions on Knowledge and Data Engineering}, 21(9), 1263-1284.

  \bibitem{student-analytics}
  Romero, C., \& Ventura, S. (2010). Educational Data Mining: A Review of the State of the Art. \textit{IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 40(6), 601-618.

  \bibitem{gridsearch}
  Bergstra, J., \& Bengio, Y. (2012). Random search for hyper-parameter optimization. \textit{Journal of Machine Learning Research}, 13, 281-305.
\end{thebibliography}
\end{document}